import numpy as np
import pandas as pd
import streamlit as st
import time
import os
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from github import Github

# ç¯å¢ƒé…ç½®æ ¡éªŒ
if not os.getenv("GITHUB_TOKEN"):
    st.error("GitHub Tokenæœªé…ç½®ï¼Œè¯·åœ¨Secretsä¸­è®¾ç½®GITHUB_TOKEN")
    st.stop()

# GitHubä»“åº“é…ç½®
REPO_NAME = "xantoxia/neckv4"
MODELS_DIR = "models/"
DATA_DIR = "data/"
COMMIT_MSG_MODEL = "æ¨¡å‹æ–‡ä»¶æ›´æ–°"
COMMIT_MSG_DATA = "ç”¨æˆ·æ•°æ®ä¸Šä¼ "

# ========== GitHubæ“ä½œå‡½æ•° ==========
def upload_model_to_github(file_path, github_path):
    """æ¨¡å‹æ–‡ä»¶ä¸“ç”¨ä¸Šä¼ å‡½æ•°ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰"""
    try:
        g = Github(os.getenv("GITHUB_TOKEN"))
        repo = g.get_repo(REPO_NAME)
        
        with open(file_path, "rb") as f:
            content = f.read()
            
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        try:
            file = repo.get_contents(github_path)
            repo.update_file(github_path, COMMIT_MSG_MODEL, content, file.sha)
        except:
            repo.create_file(github_path, COMMIT_MSG_MODEL, content)
            
        st.success(f"æ¨¡å‹ {github_path} ä¸Šä¼ æˆåŠŸ")
        return True
    except Exception as e:
        st.error(f"æ¨¡å‹ä¸Šä¼ å¤±è´¥: {str(e)}")
        return False

def upload_csv_to_github(uploaded_file):
    """CSVæ•°æ®ä¸“ç”¨ä¸Šä¼ å‡½æ•°ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰"""
    try:
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        github_path = f"{DATA_DIR}{timestamp}_{uploaded_file.name}"
        content = uploaded_file.getvalue()  # ç›´æ¥è·å–å­—èŠ‚æµâ€Œ:ml-citation{ref="1,2" data="citationList"}
        
        g = Github(os.getenv("GITHUB_TOKEN"))
        repo = g.get_repo(REPO_NAME)
        repo.create_file(github_path, COMMIT_MSG_DATA, content)
        
        st.success(f"CSVæ–‡ä»¶å·²å­˜æ¡£è‡³ {github_path}")
        return True
    except Exception as e:
        st.error(f"CSVä¸Šä¼ å¤±è´¥: {str(e)}")
        return False

# ========== æ•°æ®å¤„ç†å‡½æ•° ==========
def process_uploaded_data(uploaded_file):
    """CSVæ•°æ®å¤„ç†æµç¨‹"""
    try:
        df = pd.read_csv(uploaded_file)
        # æ·»åŠ æ•°æ®å¤„ç†é€»è¾‘...
        return df
    except Exception as e:
        st.error(f"æ•°æ®å¤„ç†é”™è¯¯: {str(e)}")
        return None

# ========== ä¸»ç•Œé¢ ==========
def main():
    st.title("æ•°æ®åˆ†æä¸æ¨¡å‹ç®¡ç†å¹³å°")
    
    # ä¾§è¾¹æ æ¨¡å—
    with st.sidebar:
        st.header("æ–‡ä»¶ä¸Šä¼ ")
        uploaded_file = st.file_uploader("ä¸Šä¼ CSVæ–‡ä»¶", type=["csv"])
        
        if uploaded_file:
            if upload_csv_to_github(uploaded_file):  # è‡ªåŠ¨è§¦å‘ä¸Šä¼ â€Œ:ml-citation{ref="1,4" data="citationList"}
                process_uploaded_data(uploaded_file)
        
        # æ¨¡å‹è®­ç»ƒæ¨¡å—
        if st.button("è®­ç»ƒæ–°æ¨¡å‹"):
            timestamp = time.strftime("%Y%m%d-%H%M%S")
            model_path = f"/tmp/MSD-{timestamp}.joblib"
            # æ·»åŠ æ¨¡å‹è®­ç»ƒé€»è¾‘...
            upload_model_to_github(model_path, f"{MODELS_DIR}MSD-{timestamp}.joblib")

if __name__ == "__main__":
    main()

# GitHub é…ç½®
repo_name = "xantoxia/neckv4"  # æ›¿æ¢ä¸ºä½ çš„ GitHub ä»“åº“
models_folder = "models/"  # GitHub ä»“åº“ä¸­æ¨¡å‹æ–‡ä»¶å­˜å‚¨è·¯å¾„
latest_model_file = "latest_model_info.txt"  # æœ€æ–°æ¨¡å‹ä¿¡æ¯æ–‡ä»¶
commit_message = "ä»Streamlitæ›´æ–°æ¨¡å‹æ–‡ä»¶"  # æäº¤ä¿¡æ¯

# å®šä¹‰å¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶å
timestamp = time.strftime("%Y%m%d-%H%M%S")
model_filename = f"MSD-{timestamp}.joblib"

# ä¸Šä¼ æ–‡ä»¶åˆ° GitHub
def upload_file_to_github(file_path, github_path, commit_message):
    try:
        g = Github(token)
        repo = g.get_repo(repo_name)

        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(file_path, "rb") as f:
            content = f.read()

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        try:
            file = repo.get_contents(github_path)
            repo.update_file(github_path, commit_message, content, file.sha)
            st.success(f"æ–‡ä»¶å·²æˆåŠŸæ›´æ–°åˆ° GitHub ä»“åº“ï¼š{github_path}")
        except:
            repo.create_file(github_path, commit_message, content)
            st.success(f"æ–‡ä»¶å·²æˆåŠŸä¸Šä¼ åˆ° GitHub ä»“åº“ï¼š{github_path}")
    except Exception as e:
        st.error(f"ä¸Šä¼ æ–‡ä»¶åˆ° GitHub å¤±è´¥ï¼š{e}")

# ä¸‹è½½æœ€æ–°æ¨¡å‹æ–‡ä»¶
def download_latest_model_from_github():
    try:
        g = Github(token)
        repo = g.get_repo(repo_name)

        # è·å–æœ€æ–°æ¨¡å‹ä¿¡æ¯
        try:
            latest_info = repo.get_contents(models_folder + latest_model_file).decoded_content.decode()
            latest_model_path = models_folder + latest_info.strip()
            st.write(f"æœ€æ–°æ¨¡å‹è·¯å¾„ï¼š{latest_model_path}")

            # ä¸‹è½½æœ€æ–°æ¨¡å‹æ–‡ä»¶
            file_content = repo.get_contents(latest_model_path)
            with open("/tmp/latest_model.joblib", "wb") as f:
                f.write(file_content.decoded_content)
            st.success("æˆåŠŸä¸‹è½½æœ€æ–°æ¨¡å‹ï¼")
            return "/tmp/latest_model.joblib"
        except:
            st.warning("æœªæ‰¾åˆ°æœ€æ–°æ¨¡å‹ä¿¡æ¯æ–‡ä»¶ï¼Œæ— æ³•ä¸‹è½½æ¨¡å‹ã€‚")
            return None
    except Exception as e:
        st.error(f"ä» GitHub ä¸‹è½½æ¨¡å‹å¤±è´¥ï¼š{e}")
        return None
        
# MSDæäº¤æ•°æ®è®°å½•  """ä¿å­˜å¹¶ä¸Šä¼ æ•°æ®åˆ°GitHub"""
def save_and_upload_data(uploaded_file):
    try:
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
        timestamp = time.strftime("%Y%m%d-%H%M%S")
        github_path = f"data/uploaded_data_{timestamp}.csv"
        
        # å°†ä¸Šä¼ æ–‡ä»¶æš‚å­˜åˆ°ä¸´æ—¶ç›®å½•
        with open(f"/tmp/{uploaded_file.name}", "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # è°ƒç”¨å·²æœ‰ä¸Šä¼ å‡½æ•°
        upload_file_to_github(
            f"/tmp/{uploaded_file.name}",
            github_path,
            "Auto-uploaded user data"
        )
        return True
    except Exception as e:
        st.error(f"æ•°æ®ä¸Šä¼ å¤±è´¥: {str(e)}")
        return False

# è®¾ç½®ä¸­æ–‡å­—ä½“
simhei_font = font_manager.FontProperties(fname="SimHei.ttf")
plt.rcParams['font.family'] = simhei_font.get_name()  # ä½¿ç”¨ SimHei å­—ä½“
plt.rcParams['axes.unicode_minus'] = False  # ä¿®å¤è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# Streamlit æ ‡é¢˜
st.title("è‚©é¢ˆè§’åº¦åˆ†æä¸å¼‚å¸¸æ£€æµ‹")
st.write("æœ¬äººå› AIå·¥å…·ç»“åˆè§„åˆ™ä¸æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¯ä»¥è‡ªåŠ¨æ£€æµ‹å¼‚å¸¸ä½œä¸šå§¿åŠ¿å¹¶æä¾›å¯è§†åŒ–åˆ†æã€‚")

# æ¨¡æ¿ä¸‹è½½
with open("è‚©é¢ˆè§’åº¦æ•°æ®æ¨¡ç‰ˆ.csv", "rb") as file:
    st.download_button(
        label="ä¸‹è½½ CSV æ¨¡æ¿",
        data=file,
        file_name="template.csv",
        mime="text/csv"
    )

# æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
uploaded_file = st.file_uploader("ä¸Šä¼ è‚©é¢ˆè§’åº¦æ•°æ®æ–‡ä»¶ (CSV æ ¼å¼)", type="csv")

# ä¿å­˜ä¸Šä¼ çš„æ•°æ®
if uploaded_file:
    # æ–°å¢æ•°æ®ä¸Šä¼ åŠŸèƒ½
    if st.sidebar.button("ğŸ“¤ ä¿å­˜æ•°æ®åˆ°GitHub"):
        if save_and_upload_data(uploaded_file):
            st.sidebar.success(f"æ•°æ®å·²å­˜æ¡£è‡³GitHubä»“åº“çš„dataç›®å½•")
            
if uploaded_file is not None:
    # æå–æ–‡ä»¶åå¹¶å»æ‰æ‰©å±•å
    csv_file_name = os.path.splitext(uploaded_file.name)[0]
     # ä½¿ç”¨ HTML æ ¼å¼è®¾ç½®å­—ä½“é¢œè‰²ä¸ºè“è‰²
    st.markdown(f"<h3 style='color:blue;'>{csv_file_name} è‚©é¢ˆä½œä¸šå§¿åŠ¿åˆ†æ</h3>", unsafe_allow_html=True)

    # è¯»å–æ•°æ®
    data = pd.read_csv(uploaded_file)
    data.columns = ['å·¥ç«™(w)', 'æ—¶é—´(s)', 'é¢ˆéƒ¨è§’åº¦(Â°)', 'è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)', 
                    'è‚©éƒ¨å¤–å±•è§’åº¦(Â°)']

    # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
    st.write("### 1.1  æ•°æ®é¢„è§ˆ")
    data_reset = data.copy()
    data_reset.index += 1
    data_reset.index.name = "åºå·"
    st.write(data_reset.head())

    # æŒ‰å·¥ç«™æ±‡æ€»è®¡ç®—
    def summarize_by_station(data):
        st.write("### 1.2  æ•°æ®ç»Ÿè®¡åˆ†æ")
    
        # æŒ‰ 'å·¥ç«™(w)' åˆ†ç»„å¹¶è®¡ç®—ç»Ÿè®¡ç‰¹æ€§
        station_summary = data.groupby('å·¥ç«™(w)').agg({
            'æ—¶é—´(s)': ['count'],
            'é¢ˆéƒ¨è§’åº¦(Â°)': ['mean', 'min', 'max', 'std'],
            'è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)': ['mean', 'min', 'max', 'std'],
            'è‚©éƒ¨å¤–å±•è§’åº¦(Â°)': ['mean', 'min', 'max', 'std']
        })

        # è°ƒæ•´åˆ—åæ ¼å¼
        station_summary.columns = ['_'.join(col).strip() for col in station_summary.columns.values]
        station_summary.reset_index(inplace=True)

        # é™åˆ¶å°æ•°ç‚¹ä½æ•°ä¸ºæœ€å¤š2ä½
        station_summary = station_summary.round(2)
  
        # æ˜¾ç¤ºæ±‡æ€»ç»Ÿè®¡ç»“æœ
        st.write(station_summary)

    # è°ƒç”¨å‡½æ•°
    summarize_by_station(data)

    def generate_visualizations(data):
        st.write("## å„å·¥ç«™æ•°æ®å¯è§†åŒ–åˆ†æ")
        
        # æŒ‰ 'å·¥ç«™(w)' åˆ†ç»„
        grouped = data.groupby('å·¥ç«™(w)')
        
        # éå†æ¯ä¸ªå·¥ç«™
        for station, group_data in grouped:
            st.write(f"### å·¥ç«™ {station} çš„æ•°æ®å¯è§†åŒ–")
            
            # ========= 1. 3D æ•£ç‚¹å›¾ =========
            st.write("#### 3D æ•£ç‚¹å›¾")
            fig = plt.figure(figsize=(10, 7))
            ax = fig.add_subplot(111, projection='3d')
            
            scatter = ax.scatter(
                group_data['æ—¶é—´(s)'], 
                group_data['é¢ˆéƒ¨è§’åº¦(Â°)'], 
                group_data['è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)'], 
                c=group_data['è‚©éƒ¨å¤–å±•è§’åº¦(Â°)'], 
                cmap='viridis'
            )
            
            # è®¾ç½®åæ ‡è½´æ ‡ç­¾
            ax.set_xlabel('æ—¶é—´(s)', fontproperties=simhei_font)
            ax.set_ylabel('é¢ˆéƒ¨è§’åº¦(Â°)', fontproperties=simhei_font)
            ax.set_zlabel('è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)', fontproperties=simhei_font)
            
            # è®¾ç½®å›¾å½¢æ ‡é¢˜
            plt.title(f'å·¥ç«™ {station} è‚©é¢ˆè§’åº¦3Då¯è§†åŒ–æ•£ç‚¹å›¾', fontproperties=simhei_font)
            
            # æ·»åŠ  colorbar
            cbar = fig.colorbar(scatter, ax=ax)
            cbar.set_label('è‚©éƒ¨å¤–å±•è§’åº¦(Â°)', fontproperties=simhei_font)
            
            # æ˜¾ç¤ºå›¾å½¢
            st.pyplot(fig)
            
            # åŠ¨æ€åˆ†æç»“è®ºï¼ˆ3Dæ•£ç‚¹å›¾ï¼‰
            st.write(f"**å·¥ç«™ {station} çš„åŠ¨æ€åˆ†æç»“è®ºï¼ˆ3Dæ•£ç‚¹å›¾ï¼‰ï¼š**")
            neck_Flexion_max = group_data['é¢ˆéƒ¨è§’åº¦(Â°)'].max()
            if neck_Flexion_max < 20:
                st.write("- ä½œä¸šæ—¶é¢ˆéƒ¨è§’åº¦å¤„äº20Â°ä¹‹å†…ï¼ŒMSDé£é™©è¾ƒä½ã€‚")
            elif 20 <= neck_Flexion_max <= 40:
                st.write("- éƒ¨åˆ†æ—¶é—´ç‚¹é¢ˆéƒ¨è§’åº¦è¶…è¿‡20Â°ï¼Œå­˜åœ¨ä¸€å®šçš„MSDé£é™©ã€‚")
            else:
                st.write("- éƒ¨åˆ†æ—¶é—´ç‚¹é¢ˆéƒ¨è§’åº¦è¶…è¿‡40Â°ï¼Œè¯·æ³¨æ„å¯èƒ½å­˜åœ¨æç«¯ä½å¤´åŠ¨ä½œã€‚")
            
            shoulder_Flexion_max = group_data['è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)'].max()
            if shoulder_Flexion_max < 15:
                st.write("- è‚©éƒ¨å‰å±ˆè§’åº¦çš„æ³¢åŠ¨è¾ƒå°ï¼ŒåŠ¨ä½œå¹…åº¦ç›¸å¯¹ä¸€è‡´ã€‚")
            elif shoulder_Flexion_max >= 45:
                st.write("- éƒ¨åˆ†æ—¶é—´ç‚¹è‚©éƒ¨å‰å±ˆè§’åº¦å¤§äº45Â°ï¼Œè¯·æ³¨æ„ä½œä¸šæ—¶æ˜¯å¦æœ‰æ‰‹éƒ¨æ”¯æ’‘ã€‚")
            
            if group_data['è‚©éƒ¨å¤–å±•è§’åº¦(Â°)'].mean() > 20:
                st.write("- è‚©éƒ¨å¤–å±•è§’åº¦çš„æ•´ä½“å¹…åº¦è¾ƒå¤§ï¼Œä¸Šè‡‚ä½œä¸šæ—¶è¿åŠ¨å¼ºåº¦å¯èƒ½è¾ƒé«˜ã€‚")
            
            # ========= 2. è‚©é¢ˆè§’åº¦æ—¶é—´å˜åŒ–æŠ˜çº¿å›¾ =========
            st.write("#### è‚©é¢ˆè§’åº¦æ—¶é—´å˜åŒ–æŠ˜çº¿å›¾ï¼ˆå¸¦æ°´å¹³é¢„è­¦çº¿ï¼‰")
            fig2, ax2 = plt.subplots(figsize=(10, 6))
            
            ax2.plot(group_data['æ—¶é—´(s)'], group_data['é¢ˆéƒ¨è§’åº¦(Â°)'], label='é¢ˆéƒ¨è§’åº¦(Â°)', color='blue', linewidth=2)
            ax2.plot(group_data['æ—¶é—´(s)'], group_data['è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)'], label='è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)', color='green', linewidth=2)
            
            # æ·»åŠ æ°´å¹³é¢„è­¦çº¿
            ax2.axhline(y=20, color='red', linestyle='--', linewidth=1.5, label='é¢ˆéƒ¨è§’åº¦é¢„è­¦çº¿ (20Â°)')
            ax2.axhline(y=45, color='orange', linestyle='--', linewidth=1.5, label='è‚©éƒ¨å‰å±ˆè§’åº¦é¢„è­¦çº¿ (45Â°)')
            
            # è®¾ç½®åæ ‡è½´å’Œæ ‡é¢˜
            ax2.set_xlabel('æ—¶é—´(s)', fontproperties=simhei_font, fontsize=12)
            ax2.set_ylabel('è§’åº¦(Â°)', fontproperties=simhei_font, fontsize=12)
            ax2.set_title(f'å·¥ç«™ {station} çš„è‚©é¢ˆè§’åº¦æ—¶é—´å˜åŒ–æŠ˜çº¿å›¾', fontproperties=simhei_font, fontsize=12)
            ax2.legend(prop=simhei_font, fontsize=10)
            
            st.pyplot(fig2)
            
            # åŠ¨æ€åˆ†æç»“è®ºï¼ˆæŠ˜çº¿å›¾ï¼‰
            st.write(f"**å·¥ç«™ {station} çš„åŠ¨æ€åˆ†æç»“è®ºï¼ˆæŠ˜çº¿å›¾ï¼‰ï¼š**")
            
            # é¢ˆéƒ¨è§’åº¦åˆ†æ
            neck_exceed_count = (group_data['é¢ˆéƒ¨è§’åº¦(Â°)'] > 20).sum()
            total_time_points = len(group_data)
            neck_exceed_ratio = neck_exceed_count / total_time_points
            
            if neck_exceed_count > 0:
                neck_risk_level = "è½»åº¦"
                neck_color = "black"
                if neck_exceed_ratio > 0.5:
                    neck_risk_level = "è¾ƒé«˜"
                    neck_color = "red"
                elif neck_exceed_ratio >= 0.25:
                    neck_risk_level = "ä¸­ç­‰"
                    neck_color = "orange"
                st.markdown(
                    f"<span style='color:{neck_color};'>- æœ‰ {neck_exceed_count} ä¸ªæ—¶é—´ç‚¹é¢ˆéƒ¨è§’åº¦è¶…è¿‡ 20Â°ï¼Œå æ¯” {neck_exceed_ratio:.2%}ï¼Œé¢ˆéƒ¨å­˜åœ¨ {neck_risk_level} MSD é£é™©ã€‚</span>", 
                    unsafe_allow_html=True
                )
            else:
                st.write("- ä½œä¸šæ—¶é¢ˆéƒ¨è§’åº¦æœªè¶…è¿‡20Â°ï¼Œé¢ˆéƒ¨MSDé£é™©è¾ƒä½ã€‚")
            
            # è‚©éƒ¨å‰å±ˆè§’åº¦åˆ†æ
            shoulder_exceed_count = (group_data['è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)'] > 45).sum()
            shoulder_exceed_ratio = shoulder_exceed_count / total_time_points
            
            if shoulder_exceed_count > 0:
                shoulder_risk_level = "è½»åº¦"
                shoulder_color = "black"
                if shoulder_exceed_ratio > 0.5:
                    shoulder_risk_level = "è¾ƒé«˜"
                    shoulder_color = "red"
                elif shoulder_exceed_ratio >= 0.25:
                    shoulder_risk_level = "ä¸­ç­‰"
                    shoulder_color = "orange"
                st.markdown(
                    f"<span style='color:{shoulder_color};'>- æœ‰ {shoulder_exceed_count} ä¸ªæ—¶é—´ç‚¹è‚©éƒ¨å‰å±ˆè§’åº¦è¶…è¿‡ 45Â°ï¼Œå æ¯” {shoulder_exceed_ratio:.2%}ï¼Œè‚©éƒ¨å­˜åœ¨ {shoulder_risk_level} MSD é£é™©ã€‚</span>",
                    unsafe_allow_html=True
                )
            else:
                st.write("- ä½œä¸šæ—¶è‚©éƒ¨å‰å±ˆè§’åº¦æœªè¶…è¿‡45Â°ï¼ŒåŠ¨ä½œå¹…åº¦è¾ƒä¸ºè‡ªç„¶ï¼Œè‚©éƒ¨MSDé£é™©è¾ƒä½ã€‚")

    # è°ƒç”¨å‡½æ•°
    generate_visualizations(data)
    
     # ç»¼åˆåˆ†æ
    def comprehensive_analysis_by_workstation(data, model):

        st.write("### 3.1  æœºå™¨å­¦ä¹ AIæ¨¡å‹åˆ†æç»“æœ")
        
        # æŒ‰ 'å·¥ç«™(w)' åˆ†ç»„
        grouped = data.groupby('å·¥ç«™(w)')

        # ç”¨äºè®°å½•æ‰€æœ‰å·¥ç«™çš„å¼‚å¸¸ç´¢å¼•
        total_abnormal_indices = []
    
        # éå†æ¯ä¸ªå·¥ç«™çš„æ•°æ®
        for station, group_data in grouped:
            st.write(f"#### å·¥ç«™{station}çš„AIæ¨¡å‹åˆ†æç»“æœ")
        
           # åŠ¨æ€é˜ˆå€¼è®¡ç®—
            neck_threshold = group_data['é¢ˆéƒ¨è§’åº¦(Â°)'].mean() + group_data['é¢ˆéƒ¨è§’åº¦(Â°)'].std()
            shoulder_threshold = group_data['è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)'].mean() + group_data['è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)'].std()

            # è¾“å‡ºåŠ¨æ€é˜ˆå€¼
            st.write(f"- **åŠ¨æ€é˜ˆå€¼**ï¼šé¢ˆéƒ¨è§’åº¦ > {neck_threshold:.2f}Â° ä¸ºå¼‚å¸¸")
            st.write(f"- **åŠ¨æ€é˜ˆå€¼**ï¼šè‚©éƒ¨å‰å±ˆ > {shoulder_threshold:.2f}Â° ä¸ºå¼‚å¸¸")

            # ç‰¹å¾é‡è¦æ€§
            st.write("##### æœºå™¨å­¦ä¹ ç‰¹å¾é‡è¦æ€§")
            feature_importances = model.feature_importances_
            for name, importance in zip(group_data.columns[2:], feature_importances):
                st.write(f"- {name}: {importance:.4f}")

            # é‡ç½®åºå·
            group_data = group_data.reset_index(drop=True)     
            
            # AIæ¨¡å‹æ£€æµ‹ç»“æœ
            abnormal_indices = []
            st.write(f"##### å·¥ç«™{station}çš„é€æ¡æ•°æ®AIåˆ†ææ£€æµ‹ç»“æœ")
        
            # å‰5æ¡
            st.write(f"###### å·¥ç«™{station}çš„å‰5æ¡æ•°æ®æ£€æµ‹ç»“æœï¼š")
            for i, row in group_data.iloc[:5].iterrows():
                rule_based_conclusion = "æ­£å¸¸"
                if row['é¢ˆéƒ¨è§’åº¦(Â°)'] > neck_threshold:
                    rule_based_conclusion = "é¢ˆéƒ¨è§’åº¦å¼‚å¸¸"
                elif row['è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)'] > shoulder_threshold:
                    rule_based_conclusion = "è‚©éƒ¨å‰å±ˆè§’åº¦å¼‚å¸¸"

                ml_conclusion = "å¼‚å¸¸" if model.predict([[row['é¢ˆéƒ¨è§’åº¦(Â°)'], row['è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)'], 
                                                          row['è‚©éƒ¨å¤–å±•è§’åº¦(Â°)']]])[0] == 1 else "æ­£å¸¸"

                if rule_based_conclusion == "æ­£å¸¸" and ml_conclusion == "å¼‚å¸¸":
                    st.write(f"- ç¬¬ {i+1} æ¡æ•°æ®ï¼šæœºå™¨å­¦ä¹ æ£€æµ‹ä¸ºå¼‚å¸¸å§¿åŠ¿ï¼Œä½†è§„åˆ™æœªå‘ç°ï¼Œå»ºè®®è¿›ä¸€æ­¥åˆ†æã€‚")
                    abnormal_indices.append(i)
                elif rule_based_conclusion != "æ­£å¸¸" and ml_conclusion == "å¼‚å¸¸":
                    st.write(f"- ç¬¬ {i+1} æ¡æ•°æ®ï¼šè§„åˆ™ä¸æœºå™¨å­¦ä¹ ä¸€è‡´æ£€æµ‹ä¸ºå¼‚å¸¸å§¿åŠ¿ï¼Œé—®é¢˜å¯èƒ½è¾ƒä¸¥é‡ã€‚")
                    abnormal_indices.append(i)
                elif rule_based_conclusion != "æ­£å¸¸" and ml_conclusion == "æ­£å¸¸":
                    st.write(f"- ç¬¬ {i+1} æ¡æ•°æ®ï¼šè§„åˆ™æ£€æµ‹ä¸ºå¼‚å¸¸å§¿åŠ¿ï¼Œä½†æœºå™¨å­¦ä¹ æœªæ£€æµ‹ä¸ºå¼‚å¸¸ï¼Œå»ºè®®è¯„ä¼°è§„åˆ™çš„é€‚ç”¨æ€§ã€‚")
                    abnormal_indices.append(i)
                else:
                    st.write(f"- ç¬¬ {i+1} æ¡æ•°æ®ï¼šè§„åˆ™å’Œæœºå™¨å­¦ä¹ å‡æ£€æµ‹ä¸ºæ­£å¸¸å§¿åŠ¿ï¼Œæ— æ˜æ˜¾é—®é¢˜ã€‚")
        
            # ä¸­é—´æ•°æ®æŠ˜å 
            if len(group_data) > 10:
                st.write(f"###### å·¥ç«™{station}çš„ä¸­é—´æ•°æ®æ£€æµ‹ç»“æœï¼š")
                with st.expander(f"å±•å¼€æŸ¥çœ‹å·¥ç«™{station}çš„ä¸­é—´æ•°æ®æ£€æµ‹ç»“æœ"):
                    for i, row in group_data.iloc[5:-5].iterrows():
                        rule_based_conclusion = "æ­£å¸¸"
                        if row['é¢ˆéƒ¨è§’åº¦(Â°)'] > neck_threshold:
                            rule_based_conclusion = "é¢ˆéƒ¨è§’åº¦å¼‚å¸¸"
                        elif row['è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)'] > shoulder_threshold:
                            rule_based_conclusion = "è‚©éƒ¨å‰å±ˆè§’åº¦å¼‚å¸¸"

                        ml_conclusion = "å¼‚å¸¸" if model.predict([[row['é¢ˆéƒ¨è§’åº¦(Â°)'], row['è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)'], 
                                                              row['è‚©éƒ¨å¤–å±•è§’åº¦(Â°)']]])[0] == 1 else "æ­£å¸¸"

                        if rule_based_conclusion == "æ­£å¸¸" and ml_conclusion == "å¼‚å¸¸":
                            st.write(f"- ç¬¬ {i+1} æ¡æ•°æ®ï¼šæœºå™¨å­¦ä¹ æ£€æµ‹ä¸ºå¼‚å¸¸å§¿åŠ¿ï¼Œä½†è§„åˆ™æœªå‘ç°ï¼Œå»ºè®®è¿›ä¸€æ­¥åˆ†æã€‚")
                            abnormal_indices.append(i)
                        elif rule_based_conclusion != "æ­£å¸¸" and ml_conclusion == "å¼‚å¸¸":
                            st.write(f"- ç¬¬ {i+1} æ¡æ•°æ®ï¼šè§„åˆ™ä¸æœºå™¨å­¦ä¹ ä¸€è‡´æ£€æµ‹ä¸ºå¼‚å¸¸å§¿åŠ¿ï¼Œé—®é¢˜å¯èƒ½è¾ƒä¸¥é‡ã€‚")
                            abnormal_indices.append(i)
                        elif rule_based_conclusion != "æ­£å¸¸" and ml_conclusion == "æ­£å¸¸":
                            st.write(f"- ç¬¬ {i+1} æ¡æ•°æ®ï¼šè§„åˆ™æ£€æµ‹ä¸ºå¼‚å¸¸å§¿åŠ¿ï¼Œä½†æœºå™¨å­¦ä¹ æœªæ£€æµ‹ä¸ºå¼‚å¸¸ï¼Œå»ºè®®è¯„ä¼°è§„åˆ™çš„é€‚ç”¨æ€§ã€‚")
                            abnormal_indices.append(i)
                        else:
                            st.write(f"- ç¬¬ {i+1} æ¡æ•°æ®ï¼šè§„åˆ™å’Œæœºå™¨å­¦ä¹ å‡æ£€æµ‹ä¸ºæ­£å¸¸å§¿åŠ¿ï¼Œæ— æ˜æ˜¾é—®é¢˜ã€‚")
        
            # å5æ¡
            st.write(f"###### å·¥ç«™{station}çš„å5æ¡æ•°æ®æ£€æµ‹ç»“æœï¼š")
            for i, row in group_data.iloc[-5:].iterrows():
                rule_based_conclusion = "æ­£å¸¸"
                if row['é¢ˆéƒ¨è§’åº¦(Â°)'] > neck_threshold:
                    rule_based_conclusion = "é¢ˆéƒ¨è§’åº¦å¼‚å¸¸"
                elif row['è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)'] > shoulder_threshold:
                    rule_based_conclusion = "è‚©éƒ¨å‰å±ˆè§’åº¦å¼‚å¸¸"

                ml_conclusion = "å¼‚å¸¸" if model.predict([[row['é¢ˆéƒ¨è§’åº¦(Â°)'], row['è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)'], 
                                                          row['è‚©éƒ¨å¤–å±•è§’åº¦(Â°)']]])[0] == 1 else "æ­£å¸¸"

                if rule_based_conclusion == "æ­£å¸¸" and ml_conclusion == "å¼‚å¸¸":
                    st.write(f"- ç¬¬ {i+1} æ¡æ•°æ®ï¼šæœºå™¨å­¦ä¹ æ£€æµ‹ä¸ºå¼‚å¸¸å§¿åŠ¿ï¼Œä½†è§„åˆ™æœªå‘ç°ï¼Œå»ºè®®è¿›ä¸€æ­¥åˆ†æã€‚")
                    abnormal_indices.append(i)
                elif rule_based_conclusion != "æ­£å¸¸" and ml_conclusion == "å¼‚å¸¸":
                    st.write(f"- ç¬¬ {i+1} æ¡æ•°æ®ï¼šè§„åˆ™ä¸æœºå™¨å­¦ä¹ ä¸€è‡´æ£€æµ‹ä¸ºå¼‚å¸¸å§¿åŠ¿ï¼Œé—®é¢˜å¯èƒ½è¾ƒä¸¥é‡ã€‚")
                    abnormal_indices.append(i)
                elif rule_based_conclusion != "æ­£å¸¸" and ml_conclusion == "æ­£å¸¸":
                    st.write(f"- ç¬¬ {i+1} æ¡æ•°æ®ï¼šè§„åˆ™æ£€æµ‹ä¸ºå¼‚å¸¸å§¿åŠ¿ï¼Œä½†æœºå™¨å­¦ä¹ æœªæ£€æµ‹ä¸ºå¼‚å¸¸ï¼Œå»ºè®®è¯„ä¼°è§„åˆ™çš„é€‚ç”¨æ€§ã€‚")
                    abnormal_indices.append(i)
                else:
                    st.write(f"- ç¬¬ {i+1} æ¡æ•°æ®ï¼šè§„åˆ™å’Œæœºå™¨å­¦ä¹ å‡æ£€æµ‹ä¸ºæ­£å¸¸å§¿åŠ¿ï¼Œæ— æ˜æ˜¾é—®é¢˜ã€‚")
    
            # æ€»ç»“æ€§æè¿°
            if abnormal_indices:
                st.write(f"##### å·¥ç«™{station}æ€»ç»“ï¼šAIæ¨¡å‹å…±æ£€æµ‹åˆ° {len(abnormal_indices)} æ¡å¼‚å¸¸æ•°æ®ã€‚")
            else:
                st.write(f"##### å·¥ç«™{station}æ€»ç»“ï¼šAIæ¨¡å‹æœªæ£€æµ‹åˆ°å¼‚å¸¸æ•°æ®ã€‚")
        
             # è®°å½•å·¥ç«™å¼‚å¸¸æ•°æ®ç´¢å¼•
            total_abnormal_indices.extend(abnormal_indices)
        
        # è¿”å›æ‰€æœ‰å·¥ç«™çš„å¼‚å¸¸æ•°æ®ç´¢å¼•
        return total_abnormal_indices
  
    # æœºå™¨å­¦ä¹ 
    if uploaded_file is not None:
          
        # ä¸‹è½½æœ€æ–°æ¨¡å‹
        model_path = download_latest_model_from_github()

    if model_path:
        model = load(model_path)
        st.write("åŠ è½½æœ€æ–°æ¨¡å‹è¿›è¡Œåˆ†æ...")
    else:
        model = RandomForestClassifier(random_state=42)
        st.write("æœªåŠ è½½åˆ°æ¨¡å‹ï¼Œè®­ç»ƒæ–°æ¨¡å‹...")
    
    # æ¨¡å‹è®­ç»ƒæˆ–é‡æ–°è®­ç»ƒ
    X = data[['é¢ˆéƒ¨è§’åº¦(Â°)', 'è‚©éƒ¨å‰å±ˆè§’åº¦(Â°)', 'è‚©éƒ¨å¤–å±•è§’åº¦(Â°)']]
    if 'Label' not in data.columns:
        np.random.seed(42)
        data['Label'] = np.random.choice([0, 1], size=len(data))
    y = data['Label']

    # æ•°æ®é¢„å¤„ç†ï¼šé‡æ–°å®šä¹‰æ ‡ç­¾
    data['Label'] = ((data['é¢ˆéƒ¨è§’åº¦(Â°)'] > 20) | (data['Label'] == 1)).astype(int)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    model.fit(X_train, y_train)   
    y_pred = (model.predict_proba(X_test)[:, 1] >= 0.4).astype(int)
    y_prob = model.predict_proba(X_test)[:, 1]

    # è°ƒç”¨å‡½æ•°ç”Ÿæˆå›¾å’Œç»“è®º
    total_abnormal_indices = comprehensive_analysis_by_workstation(data, model)
    
    st.write("### 3.4  AIæ¨¡å‹è´¨é‡è¯„ä¼°")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    y_pred = (model.predict_proba(X_test)[:, 1] >= 0.4).astype(int)
    y_prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, thresholds = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)

    fig, ax = plt.subplots(figsize=(8, 6))

    # ç»˜åˆ¶ROCæ›²çº¿
    ax.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}', linestyle='-')
    ax.plot([0, 1], [0, 1], 'r--', label="éšæœºæ¨¡å‹")

    # æ‰¾åˆ°æœ€ä½³é˜ˆå€¼çš„åæ ‡
    best_threshold_index = (tpr - fpr).argmax()
    best_threshold = thresholds[best_threshold_index]
    best_fpr = fpr[best_threshold_index]
    best_tpr = tpr[best_threshold_index]

    # åœ¨ROCæ›²çº¿ä¸Šæ ‡æ³¨æœ€ä½³é˜ˆå€¼ç‚¹
    ax.scatter(best_fpr, best_tpr, color='red', label=f'æœ€ä½³é˜ˆå€¼: {best_threshold:.2f}')
    ax.annotate(f'({best_fpr:.2f}, {best_tpr:.2f})',
                xy=(best_fpr, best_tpr),
                xytext=(best_fpr - 0.2, best_tpr - 0.1),
                arrowprops=dict(facecolor='red', arrowstyle='->'),
                fontsize=10,
                fontproperties=simhei_font)

    # è®¾ç½®æ ‡é¢˜å’Œè½´æ ‡ç­¾
    ax.set_xlabel('å‡é˜³æ€§ç‡', fontproperties=simhei_font)
    ax.set_ylabel('çœŸé˜³æ€§ç‡', fontproperties=simhei_font)
    ax.set_title('ROCæ›²çº¿', fontproperties=simhei_font)

    # æ·»åŠ å›¾ä¾‹ï¼Œæ˜¾å¼è®¾ç½®å­—ä½“
    ax.legend(loc='lower right', prop=simhei_font)

    # åœ¨Streamlitä¸­æ˜¾ç¤ºå›¾åƒ
    st.pyplot(fig)
     
    st.write("\n**AIæ¨¡å‹ä¼˜åŒ–å»ºè®®**")
    st.write(f"AIæ¨¡å‹AUCå€¼ä¸º {roc_auc:.2f}ï¼Œæœ€ä½³é˜ˆå€¼ä¸º {best_threshold:.2f}ï¼Œå¯æ ¹æ®æ­¤é˜ˆå€¼ä¼˜åŒ–AIæ¨¡å‹ã€‚")
    
     # ä¿å­˜æ–°æ¨¡å‹åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹
    local_model_path = f"/tmp/{model_filename}"
    dump(model, local_model_path)
    st.write("æ¨¡å‹å·²è®­ç»ƒå¹¶ä¿å­˜åˆ°æœ¬åœ°ä¸´æ—¶è·¯å¾„ã€‚")

    # ä¸Šä¼ æ–°æ¨¡å‹åˆ° GitHub
    upload_file_to_github(local_model_path, models_folder + model_filename, commit_message)
    st.write("æ¨¡å‹å·²ä¿å­˜å¹¶ä¸Šä¼ åˆ° GitHubã€‚")
    
    # æ›´æ–°æœ€æ–°æ¨¡å‹ä¿¡æ¯
    latest_info_path = "/tmp/" + latest_model_file
    with open(latest_info_path, "w") as f:
        f.write(model_filename)
    upload_file_to_github(latest_info_path, models_folder + latest_model_file, "æ›´æ–°æœ€æ–°æ¨¡å‹ä¿¡æ¯")
    st.success("æ–°æ¨¡å‹å·²ä¸Šä¼ ï¼Œå¹¶æ›´æ–°æœ€æ–°æ¨¡å‹è®°å½•ã€‚")

    st.write("#### é¡µé¢å¯¼å‡º")
    st.info("å¦‚éœ€å¯¼å‡ºé¡µé¢ä¸º html æ–‡ä»¶ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­æŒ‰ `Ctrl+S`ï¼Œç„¶åè¿›è¡Œä¿å­˜ã€‚")
